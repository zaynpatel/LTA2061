{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f6961744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/zaynpatel/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sympy as sp\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c8223f",
   "metadata": {},
   "source": [
    "Steps in the smaller document-term example:\n",
    "\n",
    "1. Make the words and sentence lists\n",
    "2. Initialize the original document term matrix to be zero with the dimensions of our documents and words\n",
    "3. Loop through the documents and update the document term matrix with the counts of the words in the documents\n",
    "4. Apply the tf-idf methods and take the element-wise product of the tf matrix and the idf matrix\n",
    "5. Confirm that our tf-idf splitting is the same as the TfIdfVectorizer in sci-kit learn \n",
    "6. Now that we've confirmed our matrix, A, take the eigenthings of AAT and ATA, write down what these matrices mean\n",
    "7. Plot the singular vectors of U and V, plot the singular values ∑\n",
    "8. Interpret this and do the same thing again for another extreme example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "818cc437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dtm from three sentences and 5 words (should be 3x5)\n",
    "\n",
    "words = [\"elephant\", \"horse\", \"zebra\", \"donkey\", \"monkey\"]\n",
    "\n",
    "# Intention: Make one sentence random bunch of words, another that repeates, and another with some repeat of words \n",
    "documents = [\"elephant donkey zebra horse zebra monkey\",\n",
    "            \"elephant elephant elephant elephant elephant\",\n",
    "            \"horse horse horse monkey monkey horse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "446759f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "tf_idf_matrix = vectorizer.fit_transform(documents) # Learn vocab and perform idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8a3189d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.863239</td>\n",
       "      <td>1.387779e-16</td>\n",
       "      <td>elephant donkey zebra horse zebra monkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.515884</td>\n",
       "      <td>8.017837e-01</td>\n",
       "      <td>elephant elephant elephant elephant elephant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.692131</td>\n",
       "      <td>-5.976143e-01</td>\n",
       "      <td>horse horse horse monkey monkey horse</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic_1       topic_2                                     documents\n",
       "0  0.863239  1.387779e-16      elephant donkey zebra horse zebra monkey\n",
       "1  0.515884  8.017837e-01  elephant elephant elephant elephant elephant\n",
       "2  0.692131 -5.976143e-01         horse horse horse monkey monkey horse"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components = 2) # This is our k value. For now I am using k = 2 so I can plot the data in 2D.\n",
    "lsa = svd.fit_transform(tf_idf_matrix) # Apply low-rank approximation to our tf_idf_matrix\n",
    "\n",
    "topic_encoded_tfidf_df = pd.DataFrame(lsa, columns = [\"topic_1\", \"topic_2\"])\n",
    "topic_encoded_tfidf_df['documents'] = documents\n",
    "topic_encoded_tfidf_df # We can see that most variance is happening in the first sentence for topic 1 but it doesn't explain much in topic 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e71debc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['donkey', 'elephant', 'horse', 'monkey', 'zebra'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = vectorizer.get_feature_names_out()\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d31acb47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.23184364e-01,  5.15883963e-01,  5.85113263e-01,\n",
       "         3.77425362e-01,  4.46368729e-01],\n",
       "       [ 5.29378437e-17,  8.01783726e-01, -5.34522484e-01,\n",
       "        -2.67261242e-01,  1.92173424e-16]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.components_ # Only returns right singular vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91a230a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.22080427, 1.        ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.singular_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a140c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.223184</td>\n",
       "      <td>5.293784e-17</td>\n",
       "      <td>donkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.515884</td>\n",
       "      <td>8.017837e-01</td>\n",
       "      <td>elephant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.585113</td>\n",
       "      <td>-5.345225e-01</td>\n",
       "      <td>horse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.377425</td>\n",
       "      <td>-2.672612e-01</td>\n",
       "      <td>monkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.446369</td>\n",
       "      <td>1.921734e-16</td>\n",
       "      <td>zebra</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic_1       topic_2     terms\n",
       "0  0.223184  5.293784e-17    donkey\n",
       "1  0.515884  8.017837e-01  elephant\n",
       "2  0.585113 -5.345225e-01     horse\n",
       "3  0.377425 -2.672612e-01    monkey\n",
       "4  0.446369  1.921734e-16     zebra"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoding_matrix = pd.DataFrame(svd.components_, index = ['topic_1', 'topic_2']).T\n",
    "encoding_matrix[\"terms\"] = dictionary\n",
    "display(encoding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fd3d307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top concepts? Dimensions in term-space explain most of variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a51d94da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>terms</th>\n",
       "      <th>abs_topic_1</th>\n",
       "      <th>abs_topic_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.515884</td>\n",
       "      <td>8.017837e-01</td>\n",
       "      <td>elephant</td>\n",
       "      <td>0.515884</td>\n",
       "      <td>8.017837e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.585113</td>\n",
       "      <td>-5.345225e-01</td>\n",
       "      <td>horse</td>\n",
       "      <td>0.585113</td>\n",
       "      <td>5.345225e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.377425</td>\n",
       "      <td>-2.672612e-01</td>\n",
       "      <td>monkey</td>\n",
       "      <td>0.377425</td>\n",
       "      <td>2.672612e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.446369</td>\n",
       "      <td>1.921734e-16</td>\n",
       "      <td>zebra</td>\n",
       "      <td>0.446369</td>\n",
       "      <td>1.921734e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.223184</td>\n",
       "      <td>5.293784e-17</td>\n",
       "      <td>donkey</td>\n",
       "      <td>0.223184</td>\n",
       "      <td>5.293784e-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic_1       topic_2     terms  abs_topic_1   abs_topic_2\n",
       "1  0.515884  8.017837e-01  elephant     0.515884  8.017837e-01\n",
       "2  0.585113 -5.345225e-01     horse     0.585113  5.345225e-01\n",
       "3  0.377425 -2.672612e-01    monkey     0.377425  2.672612e-01\n",
       "4  0.446369  1.921734e-16     zebra     0.446369  1.921734e-16\n",
       "0  0.223184  5.293784e-17    donkey     0.223184  5.293784e-17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoding_matrix['abs_topic_1'] = np.abs(encoding_matrix['topic_1'])\n",
    "encoding_matrix['abs_topic_2'] = np.abs(encoding_matrix['topic_2'])\n",
    "display(encoding_matrix.sort_values('abs_topic_2', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f193d66a",
   "metadata": {},
   "source": [
    "### New, maybe more informative example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd1957df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Finalizing applications filed by certain immig...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. Citizenship and Immigration Services, the...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump's executive order, signed Jan. 20, title...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CBS News reported Tuesday that USCIS has direc...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The agency said in a statement attributed to a...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The statement did not address which applicatio...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Vetting on top of vetting</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>For refugees and those who have been granted a...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>“There’s a certain amount of documentation you...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>People who are granted asylum or admitted to t...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Collins said it remains to be seen how the vet...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>“So if you didn’t have a middle name ... they ...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Trump took similar steps in his previous presi...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"In slowing down those applications, because U...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The Migration Policy Institute reported in 202...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>From 2016 to 2020, spending on vetting nearly ...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>People are also vetted during the process of b...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>The pauses are being implemented just as USCIS...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence  \\\n",
       "0   Finalizing applications filed by certain immig...   \n",
       "1   U.S. Citizenship and Immigration Services, the...   \n",
       "2   Trump's executive order, signed Jan. 20, title...   \n",
       "3   CBS News reported Tuesday that USCIS has direc...   \n",
       "4   The agency said in a statement attributed to a...   \n",
       "5   The statement did not address which applicatio...   \n",
       "6                           Vetting on top of vetting   \n",
       "7   For refugees and those who have been granted a...   \n",
       "8   “There’s a certain amount of documentation you...   \n",
       "9   People who are granted asylum or admitted to t...   \n",
       "10  Collins said it remains to be seen how the vet...   \n",
       "11  “So if you didn’t have a middle name ... they ...   \n",
       "12  Trump took similar steps in his previous presi...   \n",
       "13  \"In slowing down those applications, because U...   \n",
       "14  The Migration Policy Institute reported in 202...   \n",
       "15  From 2016 to 2020, spending on vetting nearly ...   \n",
       "16  People are also vetted during the process of b...   \n",
       "17  The pauses are being implemented just as USCIS...   \n",
       "\n",
       "                                                Title  \n",
       "0   CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "1   CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "2   CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "3   CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "4   CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "5   CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "6   CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "7   CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "8   CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "9   CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "10  CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "11  CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "12  CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "13  CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "14  CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "15  CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "16  CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "17  CNBC Article: Trump Admin Stops Green Card to ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('trump1.txt', 'r') as f:\n",
    "    trump1 = f.readlines()\n",
    "    trump1 = [line.strip() for line in trump1] # Removes all \\n \n",
    "    trump1 = [s for s in trump1 if s.strip()] # Removes all empty strings\n",
    "\n",
    "sentences_df = pd.DataFrame(trump1, columns = ['Sentence'])\n",
    "sentences_df['Title'] = 'CNBC Article: Trump Admin Stops Green Card to do more vetting'\n",
    "sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12dad21f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Trump administration has paused the proces...</td>\n",
       "      <td>Guardian Article: Trump Officials Pause Greenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CBS News reported that approved refugees are p...</td>\n",
       "      <td>Guardian Article: Trump Officials Pause Greenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The move is likely to leave some immigrants gr...</td>\n",
       "      <td>Guardian Article: Trump Officials Pause Greenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“USCIS [United States Citizenship and Immigrat...</td>\n",
       "      <td>Guardian Article: Trump Officials Pause Greenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adjustment of status is the process by which i...</td>\n",
       "      <td>Guardian Article: Trump Officials Pause Greenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The DHS cited a presidential action issued by ...</td>\n",
       "      <td>Guardian Article: Trump Officials Pause Greenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>It comes as a federal judge in Manhattan on Tu...</td>\n",
       "      <td>Guardian Article: Trump Officials Pause Greenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chung, 21, has lived in the US since she was s...</td>\n",
       "      <td>Guardian Article: Trump Officials Pause Greenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chung’s case has echoes of the ongoing detenti...</td>\n",
       "      <td>Guardian Article: Trump Officials Pause Greenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>At least five students and academics of color ...</td>\n",
       "      <td>Guardian Article: Trump Officials Pause Greenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>“What we’re seeing is the use of immigration l...</td>\n",
       "      <td>Guardian Article: Trump Officials Pause Greenc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence  \\\n",
       "0   The Trump administration has paused the proces...   \n",
       "1   CBS News reported that approved refugees are p...   \n",
       "2   The move is likely to leave some immigrants gr...   \n",
       "3   “USCIS [United States Citizenship and Immigrat...   \n",
       "4   Adjustment of status is the process by which i...   \n",
       "5   The DHS cited a presidential action issued by ...   \n",
       "6   It comes as a federal judge in Manhattan on Tu...   \n",
       "7   Chung, 21, has lived in the US since she was s...   \n",
       "8   Chung’s case has echoes of the ongoing detenti...   \n",
       "9   At least five students and academics of color ...   \n",
       "10  “What we’re seeing is the use of immigration l...   \n",
       "\n",
       "                                                Title  \n",
       "0   Guardian Article: Trump Officials Pause Greenc...  \n",
       "1   Guardian Article: Trump Officials Pause Greenc...  \n",
       "2   Guardian Article: Trump Officials Pause Greenc...  \n",
       "3   Guardian Article: Trump Officials Pause Greenc...  \n",
       "4   Guardian Article: Trump Officials Pause Greenc...  \n",
       "5   Guardian Article: Trump Officials Pause Greenc...  \n",
       "6   Guardian Article: Trump Officials Pause Greenc...  \n",
       "7   Guardian Article: Trump Officials Pause Greenc...  \n",
       "8   Guardian Article: Trump Officials Pause Greenc...  \n",
       "9   Guardian Article: Trump Officials Pause Greenc...  \n",
       "10  Guardian Article: Trump Officials Pause Greenc...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('trump2.txt', 'r') as f:\n",
    "    trump2 = f.readlines()\n",
    "    trump2 = [line.strip() for line in trump2 if line.strip()]\n",
    "\n",
    "trump2_df = pd.DataFrame(trump2, columns = ['Sentence'])\n",
    "assert len(trump2) == trump2_df.shape[0]\n",
    "\n",
    "trump2_df['Title'] = 'Guardian Article: Trump Officials Pause Greencard in Crackdown'\n",
    "display(trump2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49f3d483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Finalizing applications filed by certain immig...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. Citizenship and Immigration Services, the...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump's executive order, signed Jan. 20, title...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CBS News reported Tuesday that USCIS has direc...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The agency said in a statement attributed to a...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The statement did not address which applicatio...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Vetting on top of vetting</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>For refugees and those who have been granted a...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>“There’s a certain amount of documentation you...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>People who are granted asylum or admitted to t...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Collins said it remains to be seen how the vet...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>“So if you didn’t have a middle name ... they ...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Trump took similar steps in his previous presi...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"In slowing down those applications, because U...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The Migration Policy Institute reported in 202...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>From 2016 to 2020, spending on vetting nearly ...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>People are also vetted during the process of b...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>The pauses are being implemented just as USCIS...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The Trump administration has paused the proces...</td>\n",
       "      <td>Guardian Article: Trump Officials Pause Greenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CBS News reported that approved refugees are p...</td>\n",
       "      <td>Guardian Article: Trump Officials Pause Greenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The move is likely to leave some immigrants gr...</td>\n",
       "      <td>Guardian Article: Trump Officials Pause Greenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>“USCIS [United States Citizenship and Immigrat...</td>\n",
       "      <td>Guardian Article: Trump Officials Pause Greenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Adjustment of status is the process by which i...</td>\n",
       "      <td>Guardian Article: Trump Officials Pause Greenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>The DHS cited a presidential action issued by ...</td>\n",
       "      <td>Guardian Article: Trump Officials Pause Greenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>It comes as a federal judge in Manhattan on Tu...</td>\n",
       "      <td>Guardian Article: Trump Officials Pause Greenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Chung, 21, has lived in the US since she was s...</td>\n",
       "      <td>Guardian Article: Trump Officials Pause Greenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Chung’s case has echoes of the ongoing detenti...</td>\n",
       "      <td>Guardian Article: Trump Officials Pause Greenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>At least five students and academics of color ...</td>\n",
       "      <td>Guardian Article: Trump Officials Pause Greenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>“What we’re seeing is the use of immigration l...</td>\n",
       "      <td>Guardian Article: Trump Officials Pause Greenc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence  \\\n",
       "0   Finalizing applications filed by certain immig...   \n",
       "1   U.S. Citizenship and Immigration Services, the...   \n",
       "2   Trump's executive order, signed Jan. 20, title...   \n",
       "3   CBS News reported Tuesday that USCIS has direc...   \n",
       "4   The agency said in a statement attributed to a...   \n",
       "5   The statement did not address which applicatio...   \n",
       "6                           Vetting on top of vetting   \n",
       "7   For refugees and those who have been granted a...   \n",
       "8   “There’s a certain amount of documentation you...   \n",
       "9   People who are granted asylum or admitted to t...   \n",
       "10  Collins said it remains to be seen how the vet...   \n",
       "11  “So if you didn’t have a middle name ... they ...   \n",
       "12  Trump took similar steps in his previous presi...   \n",
       "13  \"In slowing down those applications, because U...   \n",
       "14  The Migration Policy Institute reported in 202...   \n",
       "15  From 2016 to 2020, spending on vetting nearly ...   \n",
       "16  People are also vetted during the process of b...   \n",
       "17  The pauses are being implemented just as USCIS...   \n",
       "18  The Trump administration has paused the proces...   \n",
       "19  CBS News reported that approved refugees are p...   \n",
       "20  The move is likely to leave some immigrants gr...   \n",
       "21  “USCIS [United States Citizenship and Immigrat...   \n",
       "22  Adjustment of status is the process by which i...   \n",
       "23  The DHS cited a presidential action issued by ...   \n",
       "24  It comes as a federal judge in Manhattan on Tu...   \n",
       "25  Chung, 21, has lived in the US since she was s...   \n",
       "26  Chung’s case has echoes of the ongoing detenti...   \n",
       "27  At least five students and academics of color ...   \n",
       "28  “What we’re seeing is the use of immigration l...   \n",
       "\n",
       "                                                Title  \n",
       "0   CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "1   CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "2   CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "3   CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "4   CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "5   CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "6   CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "7   CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "8   CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "9   CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "10  CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "11  CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "12  CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "13  CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "14  CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "15  CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "16  CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "17  CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "18  Guardian Article: Trump Officials Pause Greenc...  \n",
       "19  Guardian Article: Trump Officials Pause Greenc...  \n",
       "20  Guardian Article: Trump Officials Pause Greenc...  \n",
       "21  Guardian Article: Trump Officials Pause Greenc...  \n",
       "22  Guardian Article: Trump Officials Pause Greenc...  \n",
       "23  Guardian Article: Trump Officials Pause Greenc...  \n",
       "24  Guardian Article: Trump Officials Pause Greenc...  \n",
       "25  Guardian Article: Trump Officials Pause Greenc...  \n",
       "26  Guardian Article: Trump Officials Pause Greenc...  \n",
       "27  Guardian Article: Trump Officials Pause Greenc...  \n",
       "28  Guardian Article: Trump Officials Pause Greenc...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "concat_df = pd.concat([sentences_df, trump2_df], ignore_index=True) # Using ignore_index = True since I want one big dataframe\n",
    "display(concat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2eadc89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let us uncover the top 20 St. Patrick’s Day activities for work. These live team-building activities will surely bring fun and purpose to your workplace before you go out to join the festivities.\n"
     ]
    }
   ],
   "source": [
    "with open('stpatricks.txt', 'r') as f:\n",
    "    stpatricks = f.readlines()\n",
    "    stp_text = [line.strip() for line in stpatricks if line.strip()]\n",
    "    print(stp_text[2])\n",
    "    \n",
    "    \n",
    "stp_df = pd.DataFrame(stp_text, columns = ['Sentence'])\n",
    "stp_df['Title'] = 'Blog: St. Patrick\\'s Day Ideas'\n",
    "#display(stp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ee442f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Finalizing applications filed by certain immig...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. Citizenship and Immigration Services, the...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump's executive order, signed Jan. 20, title...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CBS News reported Tuesday that USCIS has direc...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The agency said in a statement attributed to a...</td>\n",
       "      <td>CNBC Article: Trump Admin Stops Green Card to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>9. Lucky Leprechaun</td>\n",
       "      <td>Blog: St. Patrick's Day Ideas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>“Lucky Leprechaun” is a heartwarming game desi...</td>\n",
       "      <td>Blog: St. Patrick's Day Ideas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>In this game, participants take turns revealin...</td>\n",
       "      <td>Blog: St. Patrick's Day Ideas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>As players share their stories, they engage in...</td>\n",
       "      <td>Blog: St. Patrick's Day Ideas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>This St. Patrick’s Day activity fosters a sens...</td>\n",
       "      <td>Blog: St. Patrick's Day Ideas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence  \\\n",
       "0   Finalizing applications filed by certain immig...   \n",
       "1   U.S. Citizenship and Immigration Services, the...   \n",
       "2   Trump's executive order, signed Jan. 20, title...   \n",
       "3   CBS News reported Tuesday that USCIS has direc...   \n",
       "4   The agency said in a statement attributed to a...   \n",
       "..                                                ...   \n",
       "80                                9. Lucky Leprechaun   \n",
       "81  “Lucky Leprechaun” is a heartwarming game desi...   \n",
       "82  In this game, participants take turns revealin...   \n",
       "83  As players share their stories, they engage in...   \n",
       "84  This St. Patrick’s Day activity fosters a sens...   \n",
       "\n",
       "                                                Title  \n",
       "0   CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "1   CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "2   CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "3   CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "4   CNBC Article: Trump Admin Stops Green Card to ...  \n",
       "..                                                ...  \n",
       "80                      Blog: St. Patrick's Day Ideas  \n",
       "81                      Blog: St. Patrick's Day Ideas  \n",
       "82                      Blog: St. Patrick's Day Ideas  \n",
       "83                      Blog: St. Patrick's Day Ideas  \n",
       "84                      Blog: St. Patrick's Day Ideas  \n",
       "\n",
       "[85 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Really simple concat\n",
    "\n",
    "sentence_df = pd.concat([concat_df, stp_df], ignore_index=True) # Using ignore_index = True since I want one big dataframe\n",
    "display(sentence_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cebf01ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the words\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Vectorize just the sentence, we don't care about vectorizing the Title\n",
    "bag_of_words = vectorizer.fit_transform(sentence_df.Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96332a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components = 2)\n",
    "lsa = svd.fit_transform(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c5f5edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>sentence</th>\n",
       "      <th>Is_Trump</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010268</td>\n",
       "      <td>-0.002250</td>\n",
       "      <td>Finalizing applications filed by certain immig...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.024641</td>\n",
       "      <td>-0.003686</td>\n",
       "      <td>U.S. Citizenship and Immigration Services, the...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.018393</td>\n",
       "      <td>0.004421</td>\n",
       "      <td>Trump's executive order, signed Jan. 20, title...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.033126</td>\n",
       "      <td>-0.006623</td>\n",
       "      <td>CBS News reported Tuesday that USCIS has direc...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011679</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>The agency said in a statement attributed to a...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.191675</td>\n",
       "      <td>0.508132</td>\n",
       "      <td>9. Lucky Leprechaun</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.156631</td>\n",
       "      <td>0.207960</td>\n",
       "      <td>“Lucky Leprechaun” is a heartwarming game desi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.085967</td>\n",
       "      <td>0.124744</td>\n",
       "      <td>In this game, participants take turns revealin...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.020080</td>\n",
       "      <td>0.048248</td>\n",
       "      <td>As players share their stories, they engage in...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.369884</td>\n",
       "      <td>-0.179808</td>\n",
       "      <td>This St. Patrick’s Day activity fosters a sens...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic 1   Topic 2                                           sentence  \\\n",
       "0   0.010268 -0.002250  Finalizing applications filed by certain immig...   \n",
       "1   0.024641 -0.003686  U.S. Citizenship and Immigration Services, the...   \n",
       "2   0.018393  0.004421  Trump's executive order, signed Jan. 20, title...   \n",
       "3   0.033126 -0.006623  CBS News reported Tuesday that USCIS has direc...   \n",
       "4   0.011679  0.001345  The agency said in a statement attributed to a...   \n",
       "..       ...       ...                                                ...   \n",
       "80  0.191675  0.508132                                9. Lucky Leprechaun   \n",
       "81  0.156631  0.207960  “Lucky Leprechaun” is a heartwarming game desi...   \n",
       "82  0.085967  0.124744  In this game, participants take turns revealin...   \n",
       "83  0.020080  0.048248  As players share their stories, they engage in...   \n",
       "84  0.369884 -0.179808  This St. Patrick’s Day activity fosters a sens...   \n",
       "\n",
       "    Is_Trump  \n",
       "0       True  \n",
       "1       True  \n",
       "2       True  \n",
       "3       True  \n",
       "4       True  \n",
       "..       ...  \n",
       "80     False  \n",
       "81     False  \n",
       "82     False  \n",
       "83     False  \n",
       "84     False  \n",
       "\n",
       "[85 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic_encoded_df = pd.DataFrame(lsa, columns = ['Topic 1', 'Topic 2'])\n",
    "topic_encoded_df['sentence'] = sentence_df.Sentence\n",
    "# Is there a way to combine this?\n",
    "topic_encoded_df['Is_Trump'] = (sentence_df.Title == \"Guardian Article: Trump Officials Pause Greencard in Crackdown\")\n",
    "topic_encoded_df['Is_Trump'] = (sentence_df.Title == \"CNBC Article: Trump Admin Stops Green Card to do more vetting\")\n",
    "display(topic_encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2dac917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '11', '149', '15', '17th', '20', '2016', '2020', '2023',\n",
       "       '21', '30', '53', 'academics', 'accessories', 'accompanied',\n",
       "       'according', 'action', 'actions', 'activist', 'activities',\n",
       "       'activity', 'add', 'adding', 'additional', 'address', 'adds',\n",
       "       'adjusting', 'adjustment', 'administration', 'admission',\n",
       "       'admitted', 'adorned', 'adventure', 'affect', 'affected',\n",
       "       'agencies', 'agency', 'agenda', 'aggressively', 'aiming', 'aliens',\n",
       "       'alignment', 'amidst', 'ancestry', 'annually', 'anon', 'anonymous',\n",
       "       'anonymously', 'anticipation', 'appealing', 'applicants',\n",
       "       'application', 'applications', 'applied', 'apply', 'appreciation',\n",
       "       'appropriate', 'approved', 'area', 'arguing', 'arrange', 'asked',\n",
       "       'asking', 'assigned', 'asylee', 'asylum', 'atmosphere',\n",
       "       'attempting', 'attire', 'attorney', 'attributed', 'authorities',\n",
       "       'backlog', 'banners', 'based', 'benefits', 'billion', 'blank',\n",
       "       'blocked', 'bludgeon', 'boa', 'bonding', 'bonds', 'book', 'boost',\n",
       "       'bring', 'build', 'building', 'builds', 'bush', 'called',\n",
       "       'camaraderie', 'candies', 'capacity', 'card', 'cards', 'carried',\n",
       "       'carry', 'case', 'cbs', 'celebrate', 'celebrated', 'celebration',\n",
       "       'center', 'central', 'certain', 'chance', 'charmed', 'check',\n",
       "       'cheer', 'chocolate', 'chocolates', 'chung', 'circle', 'cited',\n",
       "       'citizenship', 'claiming', 'closely', 'closes', 'clover', 'clues',\n",
       "       'cohesive', 'coin', 'coins', 'collaborate', 'collaborative',\n",
       "       'colleague', 'colleagues', 'collins', 'color', 'colored',\n",
       "       'colorful', 'columbia', 'comb', 'come', 'comes', 'coming',\n",
       "       'communication', 'competition', 'complement', 'completion',\n",
       "       'comply', 'concerns', 'conduct', 'connection', 'connections',\n",
       "       'constitutional', 'construct', 'continues', 'contractors',\n",
       "       'coordination', 'corporate', 'correctly', 'country', 'coworkers',\n",
       "       'crackdown', 'creates', 'creating', 'criminal', 'cultural',\n",
       "       'customs', 'cut', 'day', 'decade', 'decipher', 'decorating',\n",
       "       'decors', 'decreased', 'deepen', 'degree', 'demo', 'department',\n",
       "       'deport', 'deportation', 'designated', 'designed', 'detaining',\n",
       "       'detention', 'develop', 'dhs', 'did', 'didn', 'different',\n",
       "       'directed', 'director', 'dislike', 'disqualify', 'disrupting',\n",
       "       'documentation', 'don', 'donald', 'draw', 'draws', 'drop', 'dust',\n",
       "       'earning', 'earns', 'echoes', 'economic', 'effort', 'element',\n",
       "       'embrace', 'empathy', 'employees', 'enacts', 'encounters',\n",
       "       'encouragement', 'encourages', 'encouraging', 'energetic',\n",
       "       'enforcement', 'engage', 'engages', 'engaging', 'enhances',\n",
       "       'enjoy', 'ensure', 'enter', 'entertaining', 'environment',\n",
       "       'escape', 'event', 'evidence', 'excitement', 'exciting',\n",
       "       'executive', 'exhilarating', 'experience', 'experiences',\n",
       "       'experts', 'eye', 'eyes', 'feather', 'federal', 'fee', 'festive',\n",
       "       'festivities', 'filed', 'filled', 'finalizing', 'fine', 'flag',\n",
       "       'foot', 'foreign', 'form', 'fortune', 'foster', 'fosters', 'fraud',\n",
       "       'freeze', 'friendly', 'fun', 'funded', 'game', 'gameplay', 'gaza',\n",
       "       'george', 'getting', 'goal', 'goes', 'going', 'gold', 'golden',\n",
       "       'good', 'government', 'grab', 'granted', 'green', 'growth',\n",
       "       'guardian', 'guess', 'guesses', 'guinness', 'hand', 'handles',\n",
       "       'happens', 'hardline', 'hat', 'head', 'headway', 'heartwarming',\n",
       "       'help', 'helps', 'hidden', 'hold', 'holder', 'holders', 'holding',\n",
       "       'holds', 'holiday', 'homeland', 'hoping', 'horseshoe', 'host',\n",
       "       'house', 'hunt', 'identified', 'identify', 'image', 'immersing',\n",
       "       'immersive', 'immigrants', 'immigration', 'implement',\n",
       "       'implemented', 'includes', 'increase', 'individuals', 'initiative',\n",
       "       'inside', 'instances', 'institute', 'intend', 'interaction',\n",
       "       'interactive', 'involves', 'ireland', 'irish', 'issued', 'items',\n",
       "       'jan', 'january', 'join', 'judge', 'just', 'khalil', 'know',\n",
       "       'known', 'laura', 'law', 'lawful', 'lawsuit', 'leaf', 'learn',\n",
       "       'leave', 'legal', 'leprechaun', 'let', 'letters', 'like', 'likely',\n",
       "       'limbo', 'limit', 'list', 'live', 'lived', 'lively', 'loads',\n",
       "       'locate', 'locations', 'long', 'longer', 'look', 'luck', 'lucky',\n",
       "       'mahmoud', 'maintain', 'making', 'manhattan', 'march', 'materials',\n",
       "       'maximum', 'means', 'media', 'member', 'members', 'messages',\n",
       "       'middle', 'migration', 'million', 'minimal', 'minutes', 'monday',\n",
       "       'money', 'morale', 'movement', 'mpi', 'mutual', 'mystery',\n",
       "       'national', 'nations', 'nbc', 'nearly', 'necessary', 'news',\n",
       "       'number', 'numbers', 'observation', 'observe', 'occurred',\n",
       "       'offers', 'office', 'officials', 'old', 'ongoing', 'operations',\n",
       "       'order', 'ordered', 'orders', 'overseas', 'paired', 'palestine',\n",
       "       'palestinian', 'paper', 'participants', 'participated',\n",
       "       'particularly', 'partners', 'path', 'patrick', 'patron', 'pause',\n",
       "       'paused', 'pauses', 'pending', 'people', 'perfect', 'permanent',\n",
       "       'personal', 'perspectives', 'petitions', 'pint', 'placing',\n",
       "       'player', 'players', 'plus', 'point', 'points', 'policy',\n",
       "       'political', 'portion', 'positivity', 'possible', 'pot',\n",
       "       'potential', 'predetermined', 'prepare', 'president',\n",
       "       'presidential', 'previous', 'prize', 'probably', 'problem',\n",
       "       'process', 'processing', 'promote', 'promotes', 'prompts', 'prop',\n",
       "       'protecting', 'protests', 'provide', 'provided', 'public',\n",
       "       'purpose', 'questions', 'quite', 'rainbow', 'rapid', 'read',\n",
       "       'records', 'reduced', 'reform', 'refuge', 'refugee', 'refugees',\n",
       "       'regions', 'related', 'relationships', 'remain', 'remains',\n",
       "       'report', 'reported', 'requires', 'resettlement', 'residency',\n",
       "       'resident', 'residents', 'revealing', 'revel', 'rights',\n",
       "       'rigorous', 'risks', 'room', 'safety', 'said', 'saint', 'samah',\n",
       "       'say', 'scavenger', 'score', 'screen', 'screened', 'screening',\n",
       "       'sea', 'seamlessly', 'search', 'secretly', 'security', 'seeing',\n",
       "       'seeking', 'seen', 'send', 'sense', 'serves', 'services',\n",
       "       'session', 'set', 'setup', 'seven', 'shades', 'shamrock',\n",
       "       'shamrocks', 'share', 'sharing', 'shortfall', 'shrink', 'signed',\n",
       "       'similar', 'single', 'sisay', 'sit', 'skills', 'slightly',\n",
       "       'slowing', 'small', 'smu', 'social', 'solidarity', 'solving',\n",
       "       'specific', 'speech', 'spending', 'spirit', 'spokesperson',\n",
       "       'spread', 'st', 'staff', 'start', 'statement', 'states', 'status',\n",
       "       'steps', 'stimulating', 'stories', 'storytelling', 'streamers',\n",
       "       'stronger', 'student', 'students', 'successful', 'support',\n",
       "       'supported', 'suppress', 'sure', 'surely', 'suspended', 'suspense',\n",
       "       'taking', 'tally', 'tank', 'tapestry', 'targeted', 'team',\n",
       "       'teammate', 'teams', 'teamwork', 'temporary', 'term', 'terrorists',\n",
       "       'tests', 'thematically', 'theme', 'themed', 'things', 'think',\n",
       "       'threats', 'thrill', 'time', 'titled', 'told', 'took', 'tooth',\n",
       "       'toss', 'tossing', 'traditions', 'transforming', 'treasures',\n",
       "       'treats', 'tries', 'tripled', 'trump', 'try', 'trying', 'tubes',\n",
       "       'tuesday', 'turns', 'ultimate', 'uncover', 'understanding',\n",
       "       'united', 'unity', 'universities', 'university',\n",
       "       'unpredictability', 'uplifting', 'uscis', 'use', 'using',\n",
       "       'usually', 'values', 'various', 'venerates', 'vet', 'vetted',\n",
       "       'vetting', 'vibrant', 'visa', 'visually', 'wait', 'wants', 'way',\n",
       "       'wear', 'website', 'week', 'white', 'win', 'winner', 'work',\n",
       "       'workload', 'workplace', 'worldwide', 'write', 'writing', 'year',\n",
       "       'years', 'yunseo'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dictionary = vectorizer.get_feature_names_out()\n",
    "display(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1274024c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.74957245e-02,  1.47277319e-03,  4.29120609e-04, ...,\n",
       "         1.19304647e-02,  3.41858844e-05,  5.20389045e-04],\n",
       "       [ 3.51303446e-02,  3.09957316e-03, -1.43509882e-04, ...,\n",
       "        -3.58134745e-04,  2.74174104e-04, -1.33761232e-04]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67561935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.01300013, 1.76183109])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.singular_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8c41f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.027496</td>\n",
       "      <td>0.035130</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001473</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000429</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.063895</td>\n",
       "      <td>0.180223</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011957</td>\n",
       "      <td>-0.006786</td>\n",
       "      <td>17th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>0.015422</td>\n",
       "      <td>-0.005235</td>\n",
       "      <td>write</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>0.010983</td>\n",
       "      <td>-0.000090</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>0.011930</td>\n",
       "      <td>-0.000358</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>0.000520</td>\n",
       "      <td>-0.000134</td>\n",
       "      <td>yunseo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>637 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic_1   topic_2    terms\n",
       "0    0.027496  0.035130       10\n",
       "1    0.001473  0.003100       11\n",
       "2    0.000429 -0.000144      149\n",
       "3    0.063895  0.180223       15\n",
       "4    0.011957 -0.006786     17th\n",
       "..        ...       ...      ...\n",
       "632  0.015422 -0.005235    write\n",
       "633  0.010983 -0.000090  writing\n",
       "634  0.011930 -0.000358     year\n",
       "635  0.000034  0.000274    years\n",
       "636  0.000520 -0.000134   yunseo\n",
       "\n",
       "[637 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoding_matrix = pd.DataFrame(svd.components_, index = ['topic_1', 'topic_2']).T\n",
    "\n",
    "encoding_matrix[\"terms\"] = dictionary\n",
    "display(encoding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11c7d0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>terms</th>\n",
       "      <th>abs_topic_1</th>\n",
       "      <th>abs_topic_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.326537</td>\n",
       "      <td>-0.167403</td>\n",
       "      <td>day</td>\n",
       "      <td>0.326537</td>\n",
       "      <td>0.167403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>0.318702</td>\n",
       "      <td>-0.162785</td>\n",
       "      <td>patrick</td>\n",
       "      <td>0.318702</td>\n",
       "      <td>0.162785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>0.318057</td>\n",
       "      <td>-0.161826</td>\n",
       "      <td>st</td>\n",
       "      <td>0.318057</td>\n",
       "      <td>0.161826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>0.248891</td>\n",
       "      <td>-0.020185</td>\n",
       "      <td>team</td>\n",
       "      <td>0.248891</td>\n",
       "      <td>0.020185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.200386</td>\n",
       "      <td>-0.088134</td>\n",
       "      <td>activity</td>\n",
       "      <td>0.200386</td>\n",
       "      <td>0.088134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.189415</td>\n",
       "      <td>-0.111806</td>\n",
       "      <td>hunt</td>\n",
       "      <td>0.189415</td>\n",
       "      <td>0.111806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.186644</td>\n",
       "      <td>-0.051309</td>\n",
       "      <td>fun</td>\n",
       "      <td>0.186644</td>\n",
       "      <td>0.051309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.185516</td>\n",
       "      <td>-0.040989</td>\n",
       "      <td>building</td>\n",
       "      <td>0.185516</td>\n",
       "      <td>0.040989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>0.172954</td>\n",
       "      <td>0.218063</td>\n",
       "      <td>points</td>\n",
       "      <td>0.172954</td>\n",
       "      <td>0.218063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.170764</td>\n",
       "      <td>-0.097378</td>\n",
       "      <td>scavenger</td>\n",
       "      <td>0.170764</td>\n",
       "      <td>0.097378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.146837</td>\n",
       "      <td>0.527336</td>\n",
       "      <td>leprechaun</td>\n",
       "      <td>0.146837</td>\n",
       "      <td>0.527336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.131905</td>\n",
       "      <td>-0.053467</td>\n",
       "      <td>camaraderie</td>\n",
       "      <td>0.131905</td>\n",
       "      <td>0.053467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>0.125060</td>\n",
       "      <td>0.201975</td>\n",
       "      <td>lucky</td>\n",
       "      <td>0.125060</td>\n",
       "      <td>0.201975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>0.122245</td>\n",
       "      <td>-0.060743</td>\n",
       "      <td>shamrock</td>\n",
       "      <td>0.122245</td>\n",
       "      <td>0.060743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>0.121455</td>\n",
       "      <td>0.354061</td>\n",
       "      <td>toss</td>\n",
       "      <td>0.121455</td>\n",
       "      <td>0.354061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic_1   topic_2        terms  abs_topic_1  abs_topic_2\n",
       "162  0.326537 -0.167403          day     0.326537     0.167403\n",
       "409  0.318702 -0.162785      patrick     0.318702     0.162785\n",
       "536  0.318057 -0.161826           st     0.318057     0.161826\n",
       "563  0.248891 -0.020185         team     0.248891     0.020185\n",
       "20   0.200386 -0.088134     activity     0.200386     0.088134\n",
       "295  0.189415 -0.111806         hunt     0.189415     0.111806\n",
       "251  0.186644 -0.051309          fun     0.186644     0.051309\n",
       "87   0.185516 -0.040989     building     0.185516     0.040989\n",
       "427  0.172954  0.218063       points     0.172954     0.218063\n",
       "489  0.170764 -0.097378    scavenger     0.170764     0.097378\n",
       "336  0.146837  0.527336   leprechaun     0.146837     0.527336\n",
       "91   0.131905 -0.053467  camaraderie     0.131905     0.053467\n",
       "354  0.125060  0.201975        lucky     0.125060     0.201975\n",
       "511  0.122245 -0.060743     shamrock     0.122245     0.060743\n",
       "583  0.121455  0.354061         toss     0.121455     0.354061"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoding_matrix['abs_topic_1'] = np.abs(encoding_matrix['topic_1'])\n",
    "encoding_matrix['abs_topic_2'] = np.abs(encoding_matrix['topic_2'])\n",
    "display(encoding_matrix.sort_values('abs_topic_1', ascending=False).head(15))\n",
    "\n",
    "\n",
    "# Get the .iloc of words\n",
    "\n",
    "# One of the reasons both topics seem to do with the st patricks day vs. others is because of document length?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "026420a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>terms</th>\n",
       "      <th>abs_topic_1</th>\n",
       "      <th>abs_topic_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.146837</td>\n",
       "      <td>0.527336</td>\n",
       "      <td>leprechaun</td>\n",
       "      <td>0.146837</td>\n",
       "      <td>0.527336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0.111244</td>\n",
       "      <td>0.414781</td>\n",
       "      <td>hat</td>\n",
       "      <td>0.111244</td>\n",
       "      <td>0.414781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>0.121455</td>\n",
       "      <td>0.354061</td>\n",
       "      <td>toss</td>\n",
       "      <td>0.121455</td>\n",
       "      <td>0.354061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>0.172954</td>\n",
       "      <td>0.218063</td>\n",
       "      <td>points</td>\n",
       "      <td>0.172954</td>\n",
       "      <td>0.218063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>0.125060</td>\n",
       "      <td>0.201975</td>\n",
       "      <td>lucky</td>\n",
       "      <td>0.125060</td>\n",
       "      <td>0.201975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.063895</td>\n",
       "      <td>0.180223</td>\n",
       "      <td>15</td>\n",
       "      <td>0.063895</td>\n",
       "      <td>0.180223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.326537</td>\n",
       "      <td>-0.167403</td>\n",
       "      <td>day</td>\n",
       "      <td>0.326537</td>\n",
       "      <td>0.167403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>0.318702</td>\n",
       "      <td>-0.162785</td>\n",
       "      <td>patrick</td>\n",
       "      <td>0.318702</td>\n",
       "      <td>0.162785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>0.318057</td>\n",
       "      <td>-0.161826</td>\n",
       "      <td>st</td>\n",
       "      <td>0.318057</td>\n",
       "      <td>0.161826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.093693</td>\n",
       "      <td>0.127308</td>\n",
       "      <td>coin</td>\n",
       "      <td>0.093693</td>\n",
       "      <td>0.127308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.189415</td>\n",
       "      <td>-0.111806</td>\n",
       "      <td>hunt</td>\n",
       "      <td>0.189415</td>\n",
       "      <td>0.111806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>0.051811</td>\n",
       "      <td>0.104228</td>\n",
       "      <td>participants</td>\n",
       "      <td>0.051811</td>\n",
       "      <td>0.104228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>0.061981</td>\n",
       "      <td>0.102653</td>\n",
       "      <td>rainbow</td>\n",
       "      <td>0.061981</td>\n",
       "      <td>0.102653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.170764</td>\n",
       "      <td>-0.097378</td>\n",
       "      <td>scavenger</td>\n",
       "      <td>0.170764</td>\n",
       "      <td>0.097378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0.108489</td>\n",
       "      <td>0.096580</td>\n",
       "      <td>game</td>\n",
       "      <td>0.108489</td>\n",
       "      <td>0.096580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic_1   topic_2         terms  abs_topic_1  abs_topic_2\n",
       "336  0.146837  0.527336    leprechaun     0.146837     0.527336\n",
       "277  0.111244  0.414781           hat     0.111244     0.414781\n",
       "583  0.121455  0.354061          toss     0.121455     0.354061\n",
       "427  0.172954  0.218063        points     0.172954     0.218063\n",
       "354  0.125060  0.201975         lucky     0.125060     0.201975\n",
       "3    0.063895  0.180223            15     0.063895     0.180223\n",
       "162  0.326537 -0.167403           day     0.326537     0.167403\n",
       "409  0.318702 -0.162785       patrick     0.318702     0.162785\n",
       "536  0.318057 -0.161826            st     0.318057     0.161826\n",
       "122  0.093693  0.127308          coin     0.093693     0.127308\n",
       "295  0.189415 -0.111806          hunt     0.189415     0.111806\n",
       "404  0.051811  0.104228  participants     0.051811     0.104228\n",
       "457  0.061981  0.102653       rainbow     0.061981     0.102653\n",
       "489  0.170764 -0.097378     scavenger     0.170764     0.097378\n",
       "253  0.108489  0.096580          game     0.108489     0.096580"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoding_matrix['abs_topic_1'] = np.abs(encoding_matrix['topic_1'])\n",
    "encoding_matrix['abs_topic_2'] = np.abs(encoding_matrix['topic_2'])\n",
    "display(encoding_matrix.sort_values('abs_topic_2', ascending=False).head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b614420",
   "metadata": {},
   "source": [
    "### Normalize using L2 norm to account for document length \n",
    "\n",
    "\n",
    "**Motivation:** Singular vectors of the encoding matrix don't tell us as *much* as we would like because St. Patrick's day article is twice as long as the others. So do this process again but normalize at the TfidfVectorizer() stage to see what happens. \n",
    "\n",
    "[Stanford tf-idf notes, page 32](https://web.stanford.edu/class/cs276/19handouts/lecture6-tfidf-1per.pdf) state that after normalizing the documents we get comparable weights for long/short documents. This ensures that the St. Patrick's Day document won't weigh more than the two separate Trump articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d8db9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfvectorizer = TfidfVectorizer(stop_words='english', norm='l2') # Add l2 norm to adjust for doc lengths\n",
    "sentences = tfvectorizer.fit_transform(sentence_df.Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a4fa351",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_svd = TruncatedSVD(n_components=2)\n",
    "norm_lsa = norm_svd.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b2be917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Is_Trump</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.008471</td>\n",
       "      <td>-0.003606</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.022368</td>\n",
       "      <td>-0.004589</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.018316</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.033124</td>\n",
       "      <td>-0.006547</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011538</td>\n",
       "      <td>0.002639</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.189587</td>\n",
       "      <td>0.510712</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.155845</td>\n",
       "      <td>0.209995</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.085393</td>\n",
       "      <td>0.123390</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.019755</td>\n",
       "      <td>0.051285</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.370192</td>\n",
       "      <td>-0.178844</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic 1   Topic 2  Is_Trump\n",
       "0   0.008471 -0.003606      True\n",
       "1   0.022368 -0.004589      True\n",
       "2   0.018316  0.000468      True\n",
       "3   0.033124 -0.006547      True\n",
       "4   0.011538  0.002639      True\n",
       "..       ...       ...       ...\n",
       "80  0.189587  0.510712     False\n",
       "81  0.155845  0.209995     False\n",
       "82  0.085393  0.123390     False\n",
       "83  0.019755  0.051285     False\n",
       "84  0.370192 -0.178844     False\n",
       "\n",
       "[85 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "norm_topic_encoded_df = pd.DataFrame(norm_lsa, columns = ['Topic 1', 'Topic 2'])\n",
    "topic_encoded_df['sentence'] = sentence_df.Sentence\n",
    "# Is there a way to combine this?\n",
    "norm_topic_encoded_df['Is_Trump'] = (sentence_df.Title == \"Guardian Article: Trump Officials Pause Greencard in Crackdown\")\n",
    "norm_topic_encoded_df['Is_Trump'] = (sentence_df.Title == \"CNBC Article: Trump Admin Stops Green Card to do more vetting\")\n",
    "display(norm_topic_encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79ceb0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.01304076, 1.76172142])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_svd.singular_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f925a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02641077,  0.00182001,  0.00036773, ...,  0.01271989,\n",
       "         0.00034801,  0.00033374],\n",
       "       [ 0.03669071,  0.0033582 , -0.00050152, ..., -0.00167759,\n",
       "        -0.0002879 , -0.00024332]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_svd.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f8096da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>terms</th>\n",
       "      <th>abs_topic_1</th>\n",
       "      <th>abs_topic_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.326350</td>\n",
       "      <td>-0.166740</td>\n",
       "      <td>day</td>\n",
       "      <td>0.326350</td>\n",
       "      <td>0.166740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>0.318521</td>\n",
       "      <td>-0.161702</td>\n",
       "      <td>patrick</td>\n",
       "      <td>0.318521</td>\n",
       "      <td>0.161702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>0.317879</td>\n",
       "      <td>-0.160288</td>\n",
       "      <td>st</td>\n",
       "      <td>0.317879</td>\n",
       "      <td>0.160288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>0.252337</td>\n",
       "      <td>-0.026012</td>\n",
       "      <td>team</td>\n",
       "      <td>0.252337</td>\n",
       "      <td>0.026012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.199678</td>\n",
       "      <td>-0.086428</td>\n",
       "      <td>activity</td>\n",
       "      <td>0.199678</td>\n",
       "      <td>0.086428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.190126</td>\n",
       "      <td>-0.108743</td>\n",
       "      <td>hunt</td>\n",
       "      <td>0.190126</td>\n",
       "      <td>0.108743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.187672</td>\n",
       "      <td>-0.045514</td>\n",
       "      <td>building</td>\n",
       "      <td>0.187672</td>\n",
       "      <td>0.045514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.184678</td>\n",
       "      <td>-0.053116</td>\n",
       "      <td>fun</td>\n",
       "      <td>0.184678</td>\n",
       "      <td>0.053116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>0.172987</td>\n",
       "      <td>0.217954</td>\n",
       "      <td>points</td>\n",
       "      <td>0.172987</td>\n",
       "      <td>0.217954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.171195</td>\n",
       "      <td>-0.095720</td>\n",
       "      <td>scavenger</td>\n",
       "      <td>0.171195</td>\n",
       "      <td>0.095720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.146941</td>\n",
       "      <td>0.528053</td>\n",
       "      <td>leprechaun</td>\n",
       "      <td>0.146941</td>\n",
       "      <td>0.528053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.131374</td>\n",
       "      <td>-0.052729</td>\n",
       "      <td>camaraderie</td>\n",
       "      <td>0.131374</td>\n",
       "      <td>0.052729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>0.123406</td>\n",
       "      <td>-0.062145</td>\n",
       "      <td>shamrock</td>\n",
       "      <td>0.123406</td>\n",
       "      <td>0.062145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>0.122100</td>\n",
       "      <td>0.204840</td>\n",
       "      <td>lucky</td>\n",
       "      <td>0.122100</td>\n",
       "      <td>0.204840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>0.120501</td>\n",
       "      <td>0.354988</td>\n",
       "      <td>toss</td>\n",
       "      <td>0.120501</td>\n",
       "      <td>0.354988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic_1   topic_2        terms  abs_topic_1  abs_topic_2\n",
       "162  0.326350 -0.166740          day     0.326350     0.166740\n",
       "409  0.318521 -0.161702      patrick     0.318521     0.161702\n",
       "536  0.317879 -0.160288           st     0.317879     0.160288\n",
       "563  0.252337 -0.026012         team     0.252337     0.026012\n",
       "20   0.199678 -0.086428     activity     0.199678     0.086428\n",
       "295  0.190126 -0.108743         hunt     0.190126     0.108743\n",
       "87   0.187672 -0.045514     building     0.187672     0.045514\n",
       "251  0.184678 -0.053116          fun     0.184678     0.053116\n",
       "427  0.172987  0.217954       points     0.172987     0.217954\n",
       "489  0.171195 -0.095720    scavenger     0.171195     0.095720\n",
       "336  0.146941  0.528053   leprechaun     0.146941     0.528053\n",
       "91   0.131374 -0.052729  camaraderie     0.131374     0.052729\n",
       "511  0.123406 -0.062145     shamrock     0.123406     0.062145\n",
       "354  0.122100  0.204840        lucky     0.122100     0.204840\n",
       "583  0.120501  0.354988         toss     0.120501     0.354988"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoding_matrix = pd.DataFrame(norm_svd.components_, index = ['topic_1', 'topic_2']).T\n",
    "encoding_matrix[\"terms\"] = dictionary\n",
    "encoding_matrix['abs_topic_1'] = np.abs(encoding_matrix['topic_1'])\n",
    "encoding_matrix['abs_topic_2'] = np.abs(encoding_matrix['topic_2'])\n",
    "display(encoding_matrix.sort_values('abs_topic_1', ascending=False).head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0cf5007e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>terms</th>\n",
       "      <th>abs_topic_1</th>\n",
       "      <th>abs_topic_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.146941</td>\n",
       "      <td>0.528053</td>\n",
       "      <td>leprechaun</td>\n",
       "      <td>0.146941</td>\n",
       "      <td>0.528053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0.111748</td>\n",
       "      <td>0.413610</td>\n",
       "      <td>hat</td>\n",
       "      <td>0.111748</td>\n",
       "      <td>0.413610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>0.120501</td>\n",
       "      <td>0.354988</td>\n",
       "      <td>toss</td>\n",
       "      <td>0.120501</td>\n",
       "      <td>0.354988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>0.172987</td>\n",
       "      <td>0.217954</td>\n",
       "      <td>points</td>\n",
       "      <td>0.172987</td>\n",
       "      <td>0.217954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>0.122100</td>\n",
       "      <td>0.204840</td>\n",
       "      <td>lucky</td>\n",
       "      <td>0.122100</td>\n",
       "      <td>0.204840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.065723</td>\n",
       "      <td>0.179309</td>\n",
       "      <td>15</td>\n",
       "      <td>0.065723</td>\n",
       "      <td>0.179309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.326350</td>\n",
       "      <td>-0.166740</td>\n",
       "      <td>day</td>\n",
       "      <td>0.326350</td>\n",
       "      <td>0.166740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>0.318521</td>\n",
       "      <td>-0.161702</td>\n",
       "      <td>patrick</td>\n",
       "      <td>0.318521</td>\n",
       "      <td>0.161702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>0.317879</td>\n",
       "      <td>-0.160288</td>\n",
       "      <td>st</td>\n",
       "      <td>0.317879</td>\n",
       "      <td>0.160288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.092983</td>\n",
       "      <td>0.129247</td>\n",
       "      <td>coin</td>\n",
       "      <td>0.092983</td>\n",
       "      <td>0.129247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.190126</td>\n",
       "      <td>-0.108743</td>\n",
       "      <td>hunt</td>\n",
       "      <td>0.190126</td>\n",
       "      <td>0.108743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>0.061291</td>\n",
       "      <td>0.104824</td>\n",
       "      <td>rainbow</td>\n",
       "      <td>0.061291</td>\n",
       "      <td>0.104824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>0.051863</td>\n",
       "      <td>0.100591</td>\n",
       "      <td>participants</td>\n",
       "      <td>0.051863</td>\n",
       "      <td>0.100591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.171195</td>\n",
       "      <td>-0.095720</td>\n",
       "      <td>scavenger</td>\n",
       "      <td>0.171195</td>\n",
       "      <td>0.095720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0.109073</td>\n",
       "      <td>0.093817</td>\n",
       "      <td>game</td>\n",
       "      <td>0.109073</td>\n",
       "      <td>0.093817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic_1   topic_2         terms  abs_topic_1  abs_topic_2\n",
       "336  0.146941  0.528053    leprechaun     0.146941     0.528053\n",
       "277  0.111748  0.413610           hat     0.111748     0.413610\n",
       "583  0.120501  0.354988          toss     0.120501     0.354988\n",
       "427  0.172987  0.217954        points     0.172987     0.217954\n",
       "354  0.122100  0.204840         lucky     0.122100     0.204840\n",
       "3    0.065723  0.179309            15     0.065723     0.179309\n",
       "162  0.326350 -0.166740           day     0.326350     0.166740\n",
       "409  0.318521 -0.161702       patrick     0.318521     0.161702\n",
       "536  0.317879 -0.160288            st     0.317879     0.160288\n",
       "122  0.092983  0.129247          coin     0.092983     0.129247\n",
       "295  0.190126 -0.108743          hunt     0.190126     0.108743\n",
       "457  0.061291  0.104824       rainbow     0.061291     0.104824\n",
       "404  0.051863  0.100591  participants     0.051863     0.100591\n",
       "489  0.171195 -0.095720     scavenger     0.171195     0.095720\n",
       "253  0.109073  0.093817          game     0.109073     0.093817"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoding_matrix['abs_topic_1'] = np.abs(encoding_matrix['topic_1'])\n",
    "encoding_matrix['abs_topic_2'] = np.abs(encoding_matrix['topic_2'])\n",
    "display(encoding_matrix.sort_values('abs_topic_2', ascending=False).head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302302fb",
   "metadata": {},
   "source": [
    "### Now add another Trump article - longer one from online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cd03a6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When authoritarian regimes come to power, one ...</td>\n",
       "      <td>Substack post: Trump Declares War with Immigra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This week, Trump escalated his campaign agains...</td>\n",
       "      <td>Substack post: Trump Declares War with Immigra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The message is clear: lawyers who oppose this ...</td>\n",
       "      <td>Substack post: Trump Declares War with Immigra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Framed as an effort to enforce professional et...</td>\n",
       "      <td>Substack post: Trump Declares War with Immigra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This focus makes clear that the immigrant righ...</td>\n",
       "      <td>Substack post: Trump Declares War with Immigra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>No one has filed more frivolous and unsubstant...</td>\n",
       "      <td>Substack post: Trump Declares War with Immigra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>After he lost the 2020 election, Trump sent at...</td>\n",
       "      <td>Substack post: Trump Declares War with Immigra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Those lawsuits were entirely rhetorical, not b...</td>\n",
       "      <td>Substack post: Trump Declares War with Immigra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>If anyone should be professionally disciplined...</td>\n",
       "      <td>Substack post: Trump Declares War with Immigra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Shame on you.</td>\n",
       "      <td>Substack post: Trump Declares War with Immigra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence  \\\n",
       "0   When authoritarian regimes come to power, one ...   \n",
       "1   This week, Trump escalated his campaign agains...   \n",
       "2   The message is clear: lawyers who oppose this ...   \n",
       "3   Framed as an effort to enforce professional et...   \n",
       "4   This focus makes clear that the immigrant righ...   \n",
       "..                                                ...   \n",
       "75  No one has filed more frivolous and unsubstant...   \n",
       "76  After he lost the 2020 election, Trump sent at...   \n",
       "77  Those lawsuits were entirely rhetorical, not b...   \n",
       "78  If anyone should be professionally disciplined...   \n",
       "79                                      Shame on you.   \n",
       "\n",
       "                                                Title  \n",
       "0   Substack post: Trump Declares War with Immigra...  \n",
       "1   Substack post: Trump Declares War with Immigra...  \n",
       "2   Substack post: Trump Declares War with Immigra...  \n",
       "3   Substack post: Trump Declares War with Immigra...  \n",
       "4   Substack post: Trump Declares War with Immigra...  \n",
       "..                                                ...  \n",
       "75  Substack post: Trump Declares War with Immigra...  \n",
       "76  Substack post: Trump Declares War with Immigra...  \n",
       "77  Substack post: Trump Declares War with Immigra...  \n",
       "78  Substack post: Trump Declares War with Immigra...  \n",
       "79  Substack post: Trump Declares War with Immigra...  \n",
       "\n",
       "[80 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = requests.get('https://austinkocher.substack.com/p/trump-declares-war-on-immigration')\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# New learning - how to exclude certain content from scrape\n",
    "p = soup.find_all(lambda tag: tag.name == 'p' and not tag.find_parent(class_=\"pullquote\"))\n",
    "\n",
    "clean_text = [el.text.strip() for el in p]\n",
    "clean_text = clean_text[:36] \n",
    "\n",
    "# Convert list into string so we can tokenize sentences\n",
    "text_str = \"\"\n",
    "for paragraph in clean_text:\n",
    "    text_str += paragraph\n",
    "\n",
    "sentences = sent_tokenize(text_str)\n",
    "\n",
    "trump3_df = pd.DataFrame(sentences, columns = ['Sentence'])\n",
    "trump3_df['Title'] = 'Substack post: Trump Declares War with Immigration Lawyers'\n",
    "display(trump3_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a550a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_trump_one_stp_df = pd.concat([sentence_df, trump3_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7d3a3b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      CNBC Article: Trump Admin Stops Green Card to ...\n",
       "1      CNBC Article: Trump Admin Stops Green Card to ...\n",
       "2      CNBC Article: Trump Admin Stops Green Card to ...\n",
       "3      CNBC Article: Trump Admin Stops Green Card to ...\n",
       "4      CNBC Article: Trump Admin Stops Green Card to ...\n",
       "                             ...                        \n",
       "160    Substack post: Trump Declares War with Immigra...\n",
       "161    Substack post: Trump Declares War with Immigra...\n",
       "162    Substack post: Trump Declares War with Immigra...\n",
       "163    Substack post: Trump Declares War with Immigra...\n",
       "164    Substack post: Trump Declares War with Immigra...\n",
       "Name: Title, Length: 165, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_trump_one_stp_df.Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6fb7ccc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trump2_df[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "trump2_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce154959",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', norm='l2')\n",
    "sentences = vectorizer.fit_transform(three_trump_one_stp_df.Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "879d81df",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components = 2)\n",
    "lsa = svd.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a95ca6fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>sentence</th>\n",
       "      <th>Not_Trump</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.048003</td>\n",
       "      <td>0.267070</td>\n",
       "      <td>Finalizing applications filed by certain immig...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.074011</td>\n",
       "      <td>0.336283</td>\n",
       "      <td>U.S. Citizenship and Immigration Services, the...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.046137</td>\n",
       "      <td>0.192462</td>\n",
       "      <td>Trump's executive order, signed Jan. 20, title...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.066791</td>\n",
       "      <td>0.233819</td>\n",
       "      <td>CBS News reported Tuesday that USCIS has direc...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.033302</td>\n",
       "      <td>0.179525</td>\n",
       "      <td>The agency said in a statement attributed to a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.001089</td>\n",
       "      <td>0.007243</td>\n",
       "      <td>Living in Dangerous Times</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.046396</td>\n",
       "      <td>0.244918</td>\n",
       "      <td>At the risk of understatement, I am growing in...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.068745</td>\n",
       "      <td>0.293307</td>\n",
       "      <td>I was following the reactions of various well-...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.033494</td>\n",
       "      <td>0.156344</td>\n",
       "      <td>Let’s not overlook the obvious here. No one ha...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.058074</td>\n",
       "      <td>0.308288</td>\n",
       "      <td>Moreover, U.S. attorneys right now are making ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Topic 1   Topic 2                                           sentence  \\\n",
       "0    0.048003  0.267070  Finalizing applications filed by certain immig...   \n",
       "1    0.074011  0.336283  U.S. Citizenship and Immigration Services, the...   \n",
       "2    0.046137  0.192462  Trump's executive order, signed Jan. 20, title...   \n",
       "3    0.066791  0.233819  CBS News reported Tuesday that USCIS has direc...   \n",
       "4    0.033302  0.179525  The agency said in a statement attributed to a...   \n",
       "..        ...       ...                                                ...   \n",
       "119  0.001089  0.007243                          Living in Dangerous Times   \n",
       "120  0.046396  0.244918  At the risk of understatement, I am growing in...   \n",
       "121  0.068745  0.293307  I was following the reactions of various well-...   \n",
       "122  0.033494  0.156344  Let’s not overlook the obvious here. No one ha...   \n",
       "123  0.058074  0.308288  Moreover, U.S. attorneys right now are making ...   \n",
       "\n",
       "     Not_Trump  \n",
       "0        False  \n",
       "1        False  \n",
       "2        False  \n",
       "3        False  \n",
       "4        False  \n",
       "..         ...  \n",
       "119      False  \n",
       "120      False  \n",
       "121      False  \n",
       "122      False  \n",
       "123      False  \n",
       "\n",
       "[124 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic_encoded_df = pd.DataFrame(lsa, columns = ['Topic 1', 'Topic 2'])\n",
    "topic_encoded_df['sentence'] = three_trump_one_stp_df.Sentence\n",
    "# Is there a way to combine this?\n",
    "topic_encoded_df['Not_Trump'] = (three_trump_one_stp_df.Title == \"Blog: St. Patrick\\'s Day Ideas\")\n",
    "display(topic_encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff7b3ba",
   "metadata": {},
   "source": [
    "### Decide what to do about the bottom later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ab529026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elephant': 0, 'horse': 1, 'zebra': 2, 'donkey': 3, 'monkey': 4}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lookup = { v : k for k, v in enumerate(words)}\n",
    "word_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "564f1a1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dtm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m document\u001b[38;5;241m.\u001b[39msplit():\n\u001b[1;32m      3\u001b[0m         word_idx \u001b[38;5;241m=\u001b[39m word_lookup[word]\n\u001b[0;32m----> 4\u001b[0m         dtm[d_idx][word_idx] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      5\u001b[0m dtm\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dtm' is not defined"
     ]
    }
   ],
   "source": [
    "for d_idx, document in enumerate(documents):\n",
    "    for word in document.split():\n",
    "        word_idx = word_lookup[word]\n",
    "        dtm[d_idx][word_idx] += 1\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cca84b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Divide the raw term frequencies by the total number of words in each document\n",
    "tf_matrix = dtm / dtm.sum(axis = 1, keepdims=True)\n",
    "tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0171dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_sums = np.count_nonzero(dtm, axis = 0) # We want the number of documents a term appears in. E.g. elephant appears twice\n",
    "idf_matrix = np.log(np.divide(1 + len(documents), (1 + col_sums))) + 1\n",
    "idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c23c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_matrix = tf_matrix * idf_matrix\n",
    "\n",
    "# Unsure\n",
    "norms = np.linalg.norm(tf_idf_matrix, axis = 1, keepdims=True)\n",
    "tf_idf_matrix = tf_idf_matrix / norms\n",
    "tf_idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77de0cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Confirm that this is correct with the library implementation\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(norm='l2', use_idf=True, smooth_idf=True)\n",
    "result = tfidf.fit_transform(documents)\n",
    "tfidf_matrix = result.toarray()\n",
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1976196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can confirm that my implementation and TfidfVectorizer are the same in different orders\n",
    "# The vectorizer uses alphabetical order while mine is not in alphabetical order\n",
    "tfidf.vocabulary_ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b991b2d",
   "metadata": {},
   "source": [
    "### Pause and interpret tf_idf values. Check: Do they make sense?\n",
    "\n",
    "IDF: Used to penalize common words since words that appear often across many documents don't tell us much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4238e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library implementation\n",
    "\n",
    "\n",
    "U, S, VT = np.linalg.svd(tf_idf_matrix)\n",
    "print(f\"Left Singular Vectors: {U}\")\n",
    "print(f\"Singular Values: {S}\")\n",
    "print(f\"Right Singular Vectors: {VT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dba993",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_U = pd.DataFrame(U, index = [sentence for sentence in documents], \n",
    "                   columns = [f\"Concept {i+1}\" for i in range(U.shape[1])])\n",
    "df_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984c40b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_V = pd.DataFrame(np.transpose(VT), columns = [f\"Latent Topic {i+1}\" for i in range(VT.shape[0])],\n",
    "                    index = [word for word in words])                    \n",
    "df_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72297d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_V[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9ad8bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
