{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the TF-IDF Matrix from scratch\n",
    "\n",
    "The following code includes my implementation to make the TF-IDF matrix. I compare my output with sklearn's output to confirm my intuition. \n",
    "\n",
    "**Note: If you are looking at this notebook because you came from [PracticeLSA2](https://github.com/zaynpatel/LTA2061/blob/main/PracticeLSA2.ipynb) refer to this notebook for the TF-IDF matrix calculation *instead of* the one in my original notebook. In addition, when we define the term frequency we look at the raw frequency of terms in a document divided by the number of terms in the document. This provides us a *relative frqeuency*. The intuition behind TF-IDF, in general, and the IDF definition is still correct in [PracticeLSA2](https://github.com/zaynpatel/LTA2061/blob/main/PracticeLSA2.ipynb) but the calculation of that matrix is not.**\n",
    "\n",
    "**Also note that I arranged my words alphabetically because I know this is how the TfidfVectorizer arranges its outputs. If I wanted to use my Python version I would need to sort the words before running the tf and idf steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5  0.25 0.25]\n",
      " [0.25 0.   0.75]]\n",
      "[[0.75726441 0.53215436 0.37863221]\n",
      " [0.31622777 0.         0.9486833 ]]\n",
      "[[0.75726441 0.53215436 0.37863221]\n",
      " [0.31622777 0.         0.9486833 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = ['apple apple banana kiwi', 'apple kiwi kiwi kiwi']\n",
    "words = ['apple', 'banana', 'kiwi']\n",
    "dtm = np.zeros((len(documents), len(words)))\n",
    "\n",
    "word_lookup = { v : k for k, v in enumerate(words)}\n",
    "\n",
    "for d_idx, document in enumerate(documents):\n",
    "    for word in document.split():\n",
    "        word_idx = word_lookup[word]\n",
    "        dtm[d_idx][word_idx] += 1\n",
    "        \n",
    "# Sum across the rows\n",
    "tf_matrix = dtm / dtm.sum(axis = 1, keepdims=True)\n",
    "col_sums = np.count_nonzero(dtm, axis = 0)\n",
    "idf_matrix = np.log(np.divide(1 + len(documents), (1 + col_sums))) + 1\n",
    "\n",
    "Tfidf = tf_matrix * idf_matrix\n",
    "# It turns out this normalization step is critical to getting the right answer\n",
    "norms = np.linalg.norm(Tfidf, axis = 1, keepdims=True)\n",
    "Tfidf = Tfidf / norms\n",
    "print(Tfidf)\n",
    "# Test with sklearn vectorizer\n",
    "vectorizer = TfidfVectorizer(norm='l2', use_idf=True, smooth_idf=True)\n",
    "tf_idf_matrix = vectorizer.fit_transform(documents)\n",
    "print(tf_idf_matrix.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
